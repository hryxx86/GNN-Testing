{
 "cells": [
  {
   "cell_type": "code",
   "id": "setup-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase A: Data Preparation — EODHD News Download + FinBERT Embedding\n",
    "# This notebook handles large-scale news data acquisition and embedding.\n",
    "# Run on Colab with GPU for FinBERT encoding.\n",
    "\n",
    "import os, sys, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- Environment Setup ---\n",
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DRIVE_FOLDER = \"/content/drive/MyDrive/GNN测试\"\n",
    "    os.makedirs(DRIVE_FOLDER, exist_ok=True)\n",
    "    os.chdir(DRIVE_FOLDER)\n",
    "    print(f\"Colab: working directory set to {DRIVE_FOLDER}\")\n",
    "except ImportError:\n",
    "    from pathlib import Path\n",
    "    os.chdir(Path(\"/Users/heruixi/Desktop/GNN-Testing\"))\n",
    "    print(f\"Local: working directory set to {os.getcwd()}\")\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "print(f\"Working dir: {os.getcwd()}\")\n",
    "print(f\"Data files: {[f for f in os.listdir('data') if not f.startswith('.')]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Download News from EODHD API\n",
    "\n",
    "**API Info:**\n",
    "- Endpoint: `https://eodhd.com/api/news?s={TICKER}.US`\n",
    "- Max 1000 articles per request, paginated via `offset`\n",
    "- Each request costs 5 API calls (paid plan: 100k calls/day → 20k requests/day)\n",
    "- Response includes: title, content, date, symbols, sentiment (polarity/neg/neu/pos), tags\n",
    "- Date range: 2021-01-29 to 2026-01-28 (aligned with price data)"
   ]
  },
  {
   "cell_type": "code",
   "id": "eodhd-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === EODHD News Download ===\n",
    "# Supports resume: if interrupted, re-run and it picks up where it left off.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "API_TOKEN = \"YOUR_API_TOKEN_HERE\"  # <-- Replace with your EODHD API token\n",
    "DATE_FROM = \"2021-01-29\"\n",
    "DATE_TO   = \"2026-01-28\"\n",
    "LIMIT     = 1000                    # max per request\n",
    "SLEEP     = 0.3                     # seconds between requests (stay well under 1000/min)\n",
    "OUTPUT    = \"data/sp500_news_eodhd.parquet\"\n",
    "PROGRESS  = \"data/_download_progress.json\"  # tracks completed tickers for resume\n",
    "# ----------------------------\n",
    "\n",
    "# Load S&P 500 ticker list\n",
    "sectors = pd.read_csv(\"data/sp500_sectors.csv\", index_col=0)\n",
    "all_tickers = sectors.index.tolist()\n",
    "print(f\"Total tickers to download: {len(all_tickers)}\")\n",
    "\n",
    "# Resume support: load progress\n",
    "if os.path.exists(PROGRESS):\n",
    "    with open(PROGRESS) as f:\n",
    "        progress = json.load(f)\n",
    "    done_tickers = set(progress.get(\"done\", []))\n",
    "    print(f\"Resuming: {len(done_tickers)} tickers already downloaded\")\n",
    "else:\n",
    "    progress = {\"done\": []}\n",
    "    done_tickers = set()\n",
    "\n",
    "# Collect all articles\n",
    "all_rows = []\n",
    "\n",
    "# Load existing partial data if available\n",
    "PARTIAL = \"data/_news_partial.parquet\"\n",
    "if os.path.exists(PARTIAL):\n",
    "    existing = pd.read_parquet(PARTIAL)\n",
    "    all_rows = existing.to_dict('records')\n",
    "    print(f\"Loaded {len(all_rows)} existing articles from partial save\")\n",
    "\n",
    "remaining = [t for t in all_tickers if t not in done_tickers]\n",
    "print(f\"Remaining tickers: {len(remaining)}\")\n",
    "\n",
    "errors = []\n",
    "for i, ticker in enumerate(remaining):\n",
    "    ticker_articles = 0\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        url = (\n",
    "            f\"https://eodhd.com/api/news\"\n",
    "            f\"?s={ticker}.US&from={DATE_FROM}&to={DATE_TO}\"\n",
    "            f\"&offset={offset}&limit={LIMIT}\"\n",
    "            f\"&api_token={API_TOKEN}&fmt=json\"\n",
    "        )\n",
    "        try:\n",
    "            resp = requests.get(url, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "        except Exception as e:\n",
    "            errors.append((ticker, offset, str(e)))\n",
    "            print(f\"  ERROR {ticker} offset={offset}: {e}\")\n",
    "            break\n",
    "        \n",
    "        if not data or not isinstance(data, list):\n",
    "            break\n",
    "        \n",
    "        for article in data:\n",
    "            sentiment = article.get(\"sentiment\", {}) or {}\n",
    "            all_rows.append({\n",
    "                \"date\": article.get(\"date\", \"\"),\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"content\": article.get(\"content\", \"\"),\n",
    "                \"link\": article.get(\"link\", \"\"),\n",
    "                \"symbols\": \";\".join(article.get(\"symbols\", [])),\n",
    "                \"tags\": \";\".join(article.get(\"tags\", [])),\n",
    "                \"polarity\": sentiment.get(\"polarity\", None),\n",
    "                \"neg\": sentiment.get(\"neg\", None),\n",
    "                \"neu\": sentiment.get(\"neu\", None),\n",
    "                \"pos\": sentiment.get(\"pos\", None),\n",
    "                \"query_ticker\": ticker,\n",
    "            })\n",
    "        \n",
    "        ticker_articles += len(data)\n",
    "        \n",
    "        if len(data) < LIMIT:\n",
    "            break\n",
    "        offset += LIMIT\n",
    "        time.sleep(SLEEP)\n",
    "    \n",
    "    # Mark ticker as done\n",
    "    done_tickers.add(ticker)\n",
    "    progress[\"done\"] = list(done_tickers)\n",
    "    \n",
    "    # Print progress\n",
    "    total_done = len(done_tickers)\n",
    "    if (total_done) % 10 == 0 or i == len(remaining) - 1:\n",
    "        print(f\"  [{total_done}/{len(all_tickers)}] {ticker}: {ticker_articles} articles | Total: {len(all_rows)}\")\n",
    "    \n",
    "    # Save progress every 50 tickers\n",
    "    if total_done % 50 == 0:\n",
    "        with open(PROGRESS, 'w') as f:\n",
    "            json.dump(progress, f)\n",
    "        pd.DataFrame(all_rows).to_parquet(PARTIAL, index=False)\n",
    "        print(f\"  >> Checkpoint saved ({len(all_rows)} articles)\")\n",
    "    \n",
    "    time.sleep(SLEEP)\n",
    "\n",
    "# Final save\n",
    "df_news = pd.DataFrame(all_rows)\n",
    "df_news.to_parquet(OUTPUT, index=False)\n",
    "with open(PROGRESS, 'w') as f:\n",
    "    json.dump(progress, f)\n",
    "\n",
    "print(f\"\\n=== Download Complete ===\")\n",
    "print(f\"Total articles: {len(df_news)}\")\n",
    "print(f\"Tickers completed: {len(done_tickers)}\")\n",
    "if errors:\n",
    "    print(f\"Errors ({len(errors)}): {errors[:5]}...\")\n",
    "\n",
    "# Cleanup partial files\n",
    "for f in [PARTIAL, PROGRESS]:\n",
    "    if os.path.exists(f):\n",
    "        os.remove(f)\n",
    "        print(f\"Cleaned up {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Data Validation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "id": "data-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load & Validate Downloaded News ===\n",
    "\n",
    "df_news = pd.read_parquet(\"data/sp500_news_eodhd.parquet\")\n",
    "print(f\"Raw dataset: {len(df_news)} rows, {df_news.columns.tolist()}\")\n",
    "print(f\"\\nMissing values:\\n{df_news.isna().sum()}\")\n",
    "\n",
    "# Parse dates\n",
    "df_news[\"date\"] = pd.to_datetime(df_news[\"date\"], utc=True, errors=\"coerce\")\n",
    "df_news[\"date_only\"] = df_news[\"date\"].dt.date\n",
    "\n",
    "print(f\"\\nDate range: {df_news['date'].min()} to {df_news['date'].max()}\")\n",
    "print(f\"Unique query_tickers: {df_news['query_ticker'].nunique()}\")\n",
    "\n",
    "# Articles per ticker distribution\n",
    "per_ticker = df_news.groupby(\"query_ticker\").size()\n",
    "print(f\"\\nArticles per ticker:\")\n",
    "print(f\"  Mean:   {per_ticker.mean():.0f}\")\n",
    "print(f\"  Median: {per_ticker.median():.0f}\")\n",
    "print(f\"  Min:    {per_ticker.min()} ({per_ticker.idxmin()})\")\n",
    "print(f\"  Max:    {per_ticker.max()} ({per_ticker.idxmax()})\")\n",
    "\n",
    "# Tickers with very few articles\n",
    "low_coverage = per_ticker[per_ticker < 10]\n",
    "if len(low_coverage) > 0:\n",
    "    print(f\"\\n⚠ {len(low_coverage)} tickers with <10 articles:\")\n",
    "    print(low_coverage.sort_values().head(20))\n",
    "\n",
    "# Sentiment coverage\n",
    "has_sentiment = df_news[\"polarity\"].notna().sum()\n",
    "print(f\"\\nSentiment coverage: {has_sentiment}/{len(df_news)} ({has_sentiment/len(df_news)*100:.1f}%)\")\n",
    "print(f\"Polarity stats: mean={df_news['polarity'].mean():.3f}, std={df_news['polarity'].std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "dedup-clean",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Deduplication & Cleaning ===\n",
    "\n",
    "before = len(df_news)\n",
    "\n",
    "# 1. Drop articles with no title or date\n",
    "df_news = df_news.dropna(subset=[\"title\", \"date\"])\n",
    "print(f\"After dropping null title/date: {len(df_news)} (removed {before - len(df_news)})\")\n",
    "\n",
    "# 2. Deduplicate by (title, date) — same article may appear for multiple query_tickers\n",
    "#    Keep the article but preserve all ticker associations via 'symbols' field\n",
    "before2 = len(df_news)\n",
    "df_news = df_news.drop_duplicates(subset=[\"title\", \"date\"], keep=\"first\")\n",
    "print(f\"After dedup by (title, date): {len(df_news)} (removed {before2 - len(df_news)} duplicates)\")\n",
    "\n",
    "# 3. Filter to our date range\n",
    "df_news = df_news[\n",
    "    (df_news[\"date\"] >= \"2021-01-29\") & (df_news[\"date\"] <= \"2026-01-28\")\n",
    "]\n",
    "print(f\"After date filter: {len(df_news)}\")\n",
    "\n",
    "# 4. Parse symbols into clean ticker list (remove exchange suffixes)\n",
    "def clean_symbols(sym_str):\n",
    "    \"\"\"'AAPL;AAPL.US;MSFT' -> 'AAPL;MSFT'\"\"\"\n",
    "    if not isinstance(sym_str, str) or not sym_str:\n",
    "        return \"\"\n",
    "    tickers = set()\n",
    "    for s in sym_str.split(\";\"):\n",
    "        s = s.strip()\n",
    "        if \".\" in s:\n",
    "            s = s.split(\".\")[0]  # remove .US suffix\n",
    "        if s:\n",
    "            tickers.add(s)\n",
    "    return \";\".join(sorted(tickers))\n",
    "\n",
    "df_news[\"tickers_clean\"] = df_news[\"symbols\"].apply(clean_symbols)\n",
    "\n",
    "# Save cleaned version\n",
    "df_news.to_parquet(\"data/sp500_news_clean.parquet\", index=False)\n",
    "print(f\"\\nSaved cleaned data: data/sp500_news_clean.parquet ({len(df_news)} articles)\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "build-events",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build Event-Level Dataset ===\n",
    "# Explode articles to one row per (article, ticker) pair, align with next-day return\n",
    "\n",
    "# Load price data for return computation\n",
    "prices = pd.read_csv(\"data/sp500_5y_prices.csv\", index_col=0, parse_dates=True)\n",
    "returns = prices.pct_change()\n",
    "next_ret = returns.shift(-1)  # t+1 return for label\n",
    "valid_tickers = set(prices.columns)\n",
    "print(f\"Price data: {prices.shape}, valid tickers: {len(valid_tickers)}\")\n",
    "\n",
    "# Explode: one row per (article, ticker)\n",
    "df_news[\"ticker_list\"] = df_news[\"tickers_clean\"].str.split(\";\")\n",
    "df_exploded = df_news.explode(\"ticker_list\").rename(columns={\"ticker_list\": \"ticker\"})\n",
    "df_exploded = df_exploded[df_exploded[\"ticker\"].isin(valid_tickers)]\n",
    "print(f\"After explode & filter to S&P 500: {len(df_exploded)} rows\")\n",
    "\n",
    "# Align with next trading day return\n",
    "def get_next_return(dt, ticker):\n",
    "    \"\"\"Get the next available trading day return after date dt.\"\"\"\n",
    "    try:\n",
    "        date_only = dt.normalize()  # strip time\n",
    "        future = next_ret.loc[next_ret.index > date_only, ticker]\n",
    "        if future.empty:\n",
    "            return None\n",
    "        val = future.iloc[0]\n",
    "        return val if pd.notna(val) else None\n",
    "    except (KeyError, IndexError):\n",
    "        return None\n",
    "\n",
    "print(\"Computing next-day returns (this may take a few minutes)...\")\n",
    "df_exploded[\"return_next\"] = [\n",
    "    get_next_return(dt, tk) \n",
    "    for dt, tk in zip(df_exploded[\"date\"], df_exploded[\"ticker\"])\n",
    "]\n",
    "\n",
    "# Drop rows without return data\n",
    "before = len(df_exploded)\n",
    "df_events = df_exploded.dropna(subset=[\"return_next\"]).copy()\n",
    "df_events[\"label\"] = (df_events[\"return_next\"] > 0).astype(int)\n",
    "print(f\"After return alignment: {len(df_events)} events (dropped {before - len(df_events)})\")\n",
    "\n",
    "# Select output columns\n",
    "out_cols = [\"date\", \"ticker\", \"title\", \"content\", \"polarity\", \"neg\", \"neu\", \"pos\",\n",
    "            \"tags\", \"return_next\", \"label\"]\n",
    "df_events = df_events[out_cols].reset_index(drop=True)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== Event Dataset Summary ===\")\n",
    "print(f\"Total events: {len(df_events)}\")\n",
    "print(f\"Unique tickers: {df_events['ticker'].nunique()}\")\n",
    "print(f\"Label distribution: {df_events['label'].value_counts(normalize=True).to_dict()}\")\n",
    "print(f\"Date range: {df_events['date'].min()} to {df_events['date'].max()}\")\n",
    "print(f\"\\nTop 10 tickers by count:\")\n",
    "print(df_events[\"ticker\"].value_counts().head(10))\n",
    "\n",
    "# Save\n",
    "df_events.to_parquet(\"data/sp500_news_events.parquet\", index=False)\n",
    "print(f\"\\nSaved: data/sp500_news_events.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: FinBERT Embedding\n",
    "\n",
    "Encode news titles with ProsusAI/finbert (768-dim financial text embeddings).\n",
    "- FinBERT: pre-trained on financial text (10-K, earnings reports, analyst reports)\n",
    "- 768-dim output vs MiniLM's 384-dim → 2x information capacity\n",
    "- Also extract sentiment logits (3-dim: positive/negative/neutral) as auxiliary features"
   ]
  },
  {
   "cell_type": "code",
   "id": "finbert-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Install & Load FinBERT ===\n",
    "!pip install --quiet transformers accelerate\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "MODEL_NAME = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "finbert = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME).to(device)\n",
    "finbert.eval()\n",
    "\n",
    "print(f\"FinBERT loaded: {sum(p.numel() for p in finbert.parameters())/1e6:.1f}M parameters\")\n",
    "print(f\"Hidden size: {finbert.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "finbert-encode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Batch Encode Titles with FinBERT ===\n",
    "# Extracts:\n",
    "#   1. [CLS] embedding (768-dim) from the base model\n",
    "#   2. Sentiment logits (3-dim: positive, negative, neutral) from classification head\n",
    "\n",
    "df_events = pd.read_parquet(\"data/sp500_news_events.parquet\")\n",
    "titles = df_events[\"title\"].fillna(\"\").tolist()\n",
    "print(f\"Encoding {len(titles)} titles...\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "all_embeddings = []\n",
    "all_sentiments = []  # 3-dim logits\n",
    "\n",
    "for i in range(0, len(titles), BATCH_SIZE):\n",
    "    batch = titles[i:i+BATCH_SIZE]\n",
    "    inputs = tokenizer(\n",
    "        batch, padding=True, truncation=True,\n",
    "        max_length=128, return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = finbert(**inputs, output_hidden_states=True)\n",
    "        # [CLS] token embedding from last hidden state\n",
    "        cls_emb = outputs.hidden_states[-1][:, 0, :]  # (batch, 768)\n",
    "        cls_emb = F.normalize(cls_emb, dim=1)  # L2 normalize\n",
    "        # Sentiment logits\n",
    "        sent_logits = outputs.logits  # (batch, 3): positive, negative, neutral\n",
    "        sent_probs = F.softmax(sent_logits, dim=1)\n",
    "    \n",
    "    all_embeddings.append(cls_emb.cpu().numpy())\n",
    "    all_sentiments.append(sent_probs.cpu().numpy())\n",
    "    \n",
    "    if (i // BATCH_SIZE + 1) % 50 == 0:\n",
    "        print(f\"  Encoded {i + len(batch)}/{len(titles)}\")\n",
    "\n",
    "embeddings = np.vstack(all_embeddings).astype(np.float16)\n",
    "sentiments = np.vstack(all_sentiments).astype(np.float16)\n",
    "\n",
    "print(f\"\\nEmbeddings shape: {embeddings.shape}\")\n",
    "print(f\"Sentiments shape: {sentiments.shape}\")\n",
    "print(f\"NaN check - embeddings: {np.isnan(embeddings).any()}, sentiments: {np.isnan(sentiments).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "save-embeddings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Save Embeddings & Metadata ===\n",
    "\n",
    "# Save embeddings (768-dim FinBERT)\n",
    "np.save(\"data/sp500_news_emb_finbert.npy\", embeddings)\n",
    "print(f\"Saved: data/sp500_news_emb_finbert.npy {embeddings.shape}\")\n",
    "\n",
    "# Save sentiment probabilities (3-dim: positive, negative, neutral)\n",
    "np.save(\"data/sp500_news_sentiment_finbert.npy\", sentiments)\n",
    "print(f\"Saved: data/sp500_news_sentiment_finbert.npy {sentiments.shape}\")\n",
    "\n",
    "# Save metadata\n",
    "meta = df_events[[\"date\", \"ticker\", \"label\", \"return_next\", \"polarity\"]].copy()\n",
    "meta[\"idx\"] = np.arange(len(meta))\n",
    "meta.to_parquet(\"data/sp500_news_emb_meta.parquet\", index=False)\n",
    "print(f\"Saved: data/sp500_news_emb_meta.parquet ({len(meta)} rows)\")\n",
    "\n",
    "print(f\"\\n=== Phase A Complete ===\")\n",
    "print(f\"Events:     {len(meta)}\")\n",
    "print(f\"Embedding:  {embeddings.shape[1]}-dim FinBERT\")\n",
    "print(f\"Sentiment:  3-dim (pos/neg/neu) + EODHD polarity\")\n",
    "print(f\"Label dist: {meta['label'].value_counts(normalize=True).to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4-header",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Embedding Validation"
   ]
  },
  {
   "cell_type": "code",
   "id": "embedding-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Embedding Validation ===\n",
    "\n",
    "emb = np.load(\"data/sp500_news_emb_finbert.npy\")\n",
    "sent = np.load(\"data/sp500_news_sentiment_finbert.npy\")\n",
    "meta = pd.read_parquet(\"data/sp500_news_emb_meta.parquet\")\n",
    "\n",
    "print(\"=== Integrity Checks ===\")\n",
    "print(f\"Row match: emb={emb.shape[0]}, sent={sent.shape[0]}, meta={len(meta)} → {'OK' if emb.shape[0]==sent.shape[0]==len(meta) else 'MISMATCH'}\")\n",
    "print(f\"NaN: emb={np.isnan(emb).any()}, sent={np.isnan(sent).any()}, meta={meta.isna().any().any()}\")\n",
    "print(f\"Embedding L2 norms (should be ~1.0): mean={np.linalg.norm(emb.astype(np.float32), axis=1).mean():.4f}\")\n",
    "\n",
    "print(f\"\\n=== Dataset Stats ===\")\n",
    "print(f\"Total events: {len(meta)}\")\n",
    "print(f\"Unique tickers: {meta['ticker'].nunique()}\")\n",
    "print(f\"Date range: {meta['date'].min()} to {meta['date'].max()}\")\n",
    "print(f\"Label distribution:\\n{meta['label'].value_counts()}\")\n",
    "\n",
    "# Top/bottom tickers by count\n",
    "tc = meta[\"ticker\"].value_counts()\n",
    "print(f\"\\nTop 10 tickers: {tc.head(10).to_dict()}\")\n",
    "print(f\"Bottom 10 tickers: {tc.tail(10).to_dict()}\")\n",
    "\n",
    "# FinBERT sentiment distribution\n",
    "print(f\"\\n=== FinBERT Sentiment ===\")\n",
    "labels_sent = [\"positive\", \"negative\", \"neutral\"]\n",
    "dominant = np.argmax(sent, axis=1)\n",
    "for i, lab in enumerate(labels_sent):\n",
    "    pct = (dominant == i).mean() * 100\n",
    "    print(f\"  {lab}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "tsne-validation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === t-SNE Visualization of FinBERT Embeddings ===\n",
    "# Sample 5000 points for speed, color by GICS sector\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load sector mapping\n",
    "sectors = pd.read_csv(\"data/sp500_sectors.csv\", index_col=0).squeeze().to_dict()\n",
    "\n",
    "# Sample for t-SNE\n",
    "N_SAMPLE = min(5000, len(meta))\n",
    "idx = np.random.RandomState(42).choice(len(meta), N_SAMPLE, replace=False)\n",
    "emb_sample = emb[idx].astype(np.float32)\n",
    "meta_sample = meta.iloc[idx]\n",
    "\n",
    "print(f\"Running t-SNE on {N_SAMPLE} samples...\")\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, init='pca', learning_rate='auto')\n",
    "emb_2d = tsne.fit_transform(emb_sample)\n",
    "\n",
    "# Color by sector\n",
    "meta_sample = meta_sample.copy()\n",
    "meta_sample[\"sector\"] = meta_sample[\"ticker\"].map(sectors).fillna(\"Unknown\")\n",
    "unique_sectors = sorted(meta_sample[\"sector\"].unique())\n",
    "cmap = plt.cm.get_cmap(\"tab20\", len(unique_sectors))\n",
    "color_map = {s: cmap(i) for i, s in enumerate(unique_sectors)}\n",
    "\n",
    "plt.figure(figsize=(14, 10))\n",
    "for sector in unique_sectors:\n",
    "    mask = meta_sample[\"sector\"] == sector\n",
    "    plt.scatter(emb_2d[mask, 0], emb_2d[mask, 1], \n",
    "                c=[color_map[sector]], label=sector, alpha=0.5, s=10)\n",
    "plt.legend(bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=8)\n",
    "plt.title(\"FinBERT Embeddings t-SNE (colored by GICS sector)\")\n",
    "plt.xlabel(\"Dim 1\")\n",
    "plt.ylabel(\"Dim 2\")\n",
    "plt.tight_layout()\n",
    "\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "plt.savefig(\"plots/finbert_tsne_by_sector.png\", dpi=150, bbox_inches=\"tight\")\n",
    "print(\"Saved: plots/finbert_tsne_by_sector.png\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
