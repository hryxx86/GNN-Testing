# GNN 股票相关性网络分析 — 代码逐段讲解

> 本文档面向初学者，逐段解释 `GNN测试1 colab.ipynb` 中每一个 Cell 的代码逻辑，并在最后深入讲解 HeteroData 构建和 GraphSAGE 的工作原理。

---

## Cell 0：环境初始化 + 数据下载

### 随机种子

```python
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
```

机器学习中很多操作涉及"随机"（比如初始化权重、打乱数据）。设定 `seed=42` 后，每次运行产生的随机数序列是一样的，保证结果可复现。这里把 Python 内置随机库、NumPy、PyTorch CPU 和 GPU 的种子全部锁死。

### cuDNN 确定性设置

```python
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
torch.use_deterministic_algorithms(False)
```

cuDNN 是 NVIDIA 的 GPU 加速库。`deterministic=True` 强制它用确定性算法（慢但可复现），`benchmark=False` 关掉自动选最快算法的功能（那个功能会引入不确定性）。最后一行本应开启严格确定性模式，但 Colab 的 CUDA 环境下会崩溃，所以设为 `False`。

### 自动检测运行环境

```python
try:
    import google.colab
    from google.colab import drive
    drive.mount('/content/drive')
    DRIVE_FOLDER = "/content/drive/MyDrive/GNN测试"
    os.makedirs(DRIVE_FOLDER, exist_ok=True)
    os.chdir(DRIVE_FOLDER)
except ImportError:
    notebook_dir = Path("/Users/heruixi/Desktop/GNN-Testing")
    os.chdir(notebook_dir)
```

如果在 Colab 上跑，就挂载 Google Drive 并切换到 Drive 目录（这样下次运行不用重新下载数据）。如果在本地 Mac 上跑，就切换到桌面的项目目录。`try/except` 的逻辑：Colab 里能 `import google.colab`，本地不行，就走 `except` 分支。

### 数据下载与缓存

```python
if os.path.exists(FILE_NAME):
    prices = pd.read_csv(FILE_NAME, index_col=0)
else:
    url = "https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
    html = requests.get(url, headers={'User-Agent': 'Mozilla/5.0'}).text
    tickers = pd.read_html(html)[0]['Symbol'].str.replace('.', '-', regex=False).tolist()
    data = yf.download(tickers, period="5y", auto_adjust=True)
    prices = data['Close']
    prices.to_csv(FILE_NAME)
```

先检查本地有没有已下载的 CSV：
- **有** → 直接读取，省时间
- **没有** → 从 Wikipedia 爬取 S&P 500 成分股列表，再用 `yfinance` 下载 5 年日收盘价

注意 `.str.replace('.', '-')` 是因为 Wikipedia 上的 ticker 用点号（如 `BRK.B`），但 yfinance 要求用短横线（`BRK-B`）。最终 `prices` 是一个 **1255 天 × 502 只股票** 的 DataFrame。

---

## Cell 1：保存图片的工具函数

```python
def save_graph(filename, folder="plots"):
    if not os.path.exists(folder):
        os.makedirs(folder)
    path = f"{folder}/{filename}.png"
    plt.savefig(path, dpi=300, bbox_inches='tight')
```

简单的辅助函数。如果 `plots/` 文件夹不存在就创建，然后把当前的 matplotlib 图保存为 300 DPI 的 PNG。`bbox_inches='tight'` 自动裁掉多余白边。后面每张图都会调用它。

---

## Cell 2：构建相关性图 + GCN 前向传播

### 计算日收益率

```python
returns = df.pct_change().dropna()
```

`pct_change()` 就是 `(今天价格 - 昨天价格) / 昨天价格`。第一行是 NaN（因为没有"昨天"），所以 `dropna()` 丢掉。

### 构建图（核心两行）

```python
corr_matrix = torch.tensor(returns.corr().values)
edge_index = (corr_matrix.abs() > 0.6).nonzero().t()
```

1. `returns.corr()` 计算 502×502 的 Pearson 相关系数矩阵。相关系数范围 [-1, 1]，越接近 1 说明两只股票走势越同步，越接近 -1 说明走势越相反。
2. `abs() > 0.6` 设了一个阈值——只有绝对相关性超过 0.6 的股票对之间才画一条边。`.nonzero()` 找出所有 True 的坐标，`.t()` 转置成 PyG 要求的 `[2, num_edges]` 格式。

**打个比方：** 502 只股票是 502 个人，如果两个人的走路节奏很像（相关性 > 0.6），就在他们之间拉一根线。最终得到 3198 条线。

### 定义 GCN 模型

```python
class GCN(torch.nn.Module):
    def __init__(self, in_features, hidden_dim, out_features):
        super().__init__()
        self.conv1 = GCNConv(in_features, hidden_dim)   # 1 → 16
        self.conv2 = GCNConv(hidden_dim, out_features)   # 16 → 1
```

GCN（图卷积网络）的核心思想是：每个节点把自己和邻居的特征"加权平均"起来，然后通过一个线性变换。
- `conv1`：第一层，把每个节点的 1 维特征扩展到 16 维。经过这层，每个节点的表示包含了它**一跳邻居**的信息。
- `conv2`：第二层，16 维压缩到 1 维。经过这层，信息传播到**两跳邻居**。

### 前向传播

```python
def forward(self, x, edge_index):
    x = self.conv1(x, edge_index).relu()
    x = self.conv2(x, edge_index)
    return x
```

数据流过第一层 → ReLU 激活（把负数变 0，引入非线性） → 第二层 → 输出。

### 运行模型

```python
x = torch.tensor(returns.iloc[-1].values, dtype=torch.float).view(-1, 1)
model = GCN(in_features=1, hidden_dim=16, out_features=1)
output = model(x, edge_index)
```

**输入特征：** 取最后一天的收益率作为每只股票的特征（就是一个数）。`view(-1, 1)` 把它变成 [502, 1] 的形状。

**注意：这里没有训练！** 只是用随机初始化的权重跑了一次前向传播，目的是验证整个管道能跑通，以及提取中间层的 16 维嵌入供后面 t-SNE 可视化用。

---

## Cell 3：获取市值数据

```python
for i, tk in enumerate(ticker_names_all):
    info = yf.Ticker(tk).info
    caps[tk] = info.get("marketCap", 0) or 0
```

逐个调用 yfinance API 获取每只股票的当前市值。同样有缓存机制——如果 `sp500_market_caps.csv` 已存在就直接读。市值数据后面用于筛选"前 100 大市值股票"来可视化。

---

## Cell 4：获取行业分类数据

```python
sp500_table = pd.read_html(html)[0]
sector_series = sp500_table.set_index('Symbol')['GICS Sector']
```

从 Wikipedia 的 S&P 500 表格中抓取 GICS 行业分类（11 个行业：科技、金融、医疗等）。这个映射后面用来给网络图中的节点染色——同一行业用同一颜色。

---

## Cell 5：Top 100 市值股票相关性网络图

```python
top100_tickers = market_caps.reindex(ticker_names).nlargest(100).index.tolist()
top100_indices = [ticker_names.index(tk) for tk in top100_tickers]
```

从 502 只股票中选出市值前 100 的。

```python
mask = torch.isin(edge_index[0], node_set) & torch.isin(edge_index[1], node_set)
sub_edges = edge_index[:, mask]
```

**筛选边：** 只保留"两端都在 Top 100 里"的边。不能留"一端在里面一端在外面"的边，否则图里会出现不存在的节点。

```python
g = nx.Graph()
g.add_nodes_from(sub_nodes)
g.add_edges_from(sub_edges.t().tolist())
pos = nx.spring_layout(g, seed=42, k=0.15)
nx.draw(g, pos, ...)
```

用 NetworkX 建图，`spring_layout` 是弹簧力导向布局——把有边相连的节点拉近，没有边的推远。`k=0.15` 控制节点间的理想距离。结果是一张蓝色节点、灰色边的网络图。

---

## Cell 6：行业染色版网络图

跟上一张图的逻辑完全一样，唯一区别是：

```python
sectors = [sector_map.get(ticker_names[i], "Unknown") for i in top100_indices]
cmap = plt.cm.get_cmap("tab20", len(unique_sectors))
node_colors = [sector_to_color[s] for s in sectors]
```

给每个节点按行业分配颜色。`tab20` 是 matplotlib 的一个 20 色调色板，足够区分 11 个行业。加了图例（legend）标注每种颜色对应哪个行业。**目的是验证：同行业的股票是否在图上聚在一起？** 答案是确实如此。

---

## Cell 7：全量 502 股票网络图

```python
g.add_nodes_from(range(len(returns.columns)))
g.add_edges_from(edge_index.t().tolist())
nx.draw(g, pos, node_size=100, with_labels=False, alpha=0.6)
```

把所有 502 只股票和 3198 条边全部画出来。因为太密了，所以：节点缩小（100）、关掉标签（`with_labels=False`）、半透明（`alpha=0.6`）、画布放大到 20×20 英寸。这张图的目的是看**整体结构**——哪里有密集簇，哪里有孤立节点。

---

## Cell 8：Top 10 Hub 股票及其邻居

```python
d = degree(edge_index[0], dtype=torch.long)
top_values, top_nodes_tensor = torch.topk(d, k)
```

**度（degree）= 一个节点连了多少条边。** 度最高的节点就是"枢纽"（Hub）——和最多股票有强相关性的股票。`torch.topk` 取前 10 名。

```python
mask = torch.isin(edge_index[0], top_nodes_tensor) | torch.isin(edge_index[1], top_nodes_tensor)
```

这次是 **OR 而不是 AND**——只要一条边的任意一端是 Hub，就保留。这样能看到 Hub 连接了谁。

Hub 节点画红色大圆（1000），普通邻居画蓝色小圆（100）。**关键发现：** 排名前 10 的 Hub 中 7 个是工业股、3 个是金融股。苹果、微软等市值巨头并不是 Hub——市值大 ≠ 和别人关联性强。

---

## Cell 9：t-SNE 可视化 GCN 嵌入

```python
embeddings = model.conv1(x, edge_index).detach().numpy()
```

提取 GCN **第一层的输出**（16 维向量），而不是最终输出（1 维）。`.detach()` 把张量从计算图中分离出来（不需要梯度了）。

```python
tsne = TSNE(n_components=2, perplexity=30, random_state=42)
embeddings_2d = tsne.fit_transform(embeddings)
```

**t-SNE** 是一种降维算法，把 16 维压缩到 2 维以便画散点图。`perplexity=30` 大致决定"考虑多少个近邻"。如果两只股票在 16 维空间里靠近，它们在 2D 图上也会靠近。**即使 GCN 没有训练，仅靠图的拓扑结构传播信息，就能产生有意义的聚类。**

---

## Cell 10：加载新闻事件数据

```python
events = pd.read_parquet("news_events.parquet")
events["text"] = events["text"].fillna("").str.replace(r"\s+", " ", regex=True).str.strip()
```

加载之前用 `prepare_events.py` 生成的事件数据（480 行，9 列）。`fillna("")` 填充空值，正则把多余空白压缩成单个空格。这些新闻文章将用于后面的文本嵌入。

---

## Cell 11：加载 SentenceTransformer 模型

```python
model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device=device)
```

加载一个预训练的句子嵌入模型。**all-MiniLM-L6-v2** 是 BERT 的轻量蒸馏版（6 层、2270 万参数），能把任意长度的文本变成一个 384 维的向量。语义相近的文本，向量也会相近。

---

## Cell 12：批量编码新闻文本

```python
for i in range(0, len(texts), batch_size):
    batch = texts[i:i+batch_size]
    with torch.inference_mode():
        emb = model.encode(batch, normalize_embeddings=True)
    emb_list.append(emb)
emb = np.vstack(emb_list).astype(np.float16)
```

每次取 64 篇文章送进模型，得到 384 维的嵌入向量。`torch.inference_mode()` 关闭梯度计算，省内存。`normalize_embeddings=True` 做 L2 归一化（向量长度变为 1），这样向量的点积就等于余弦相似度。最后用 `float16` 半精度存储，节省空间。输出形状：**480 × 384**。

---

## Cell 13：保存嵌入和元数据

```python
np.save("news_events_emb.npy", emb)
meta.to_parquet("news_events_emb_meta.parquet", index=False)
```

嵌入矩阵保存为 `.npy`（NumPy 二进制格式，读写快），元数据（doc_id、ticker、日期、标签等）保存为 `.parquet`（列式压缩格式）。

---

## Cell 14：完整性检查

```python
print("Row count match:", loaded_emb.shape[0] == len(loaded_meta))
print("NaN in emb:", np.isnan(loaded_emb).any())
print("Label distribution:", loaded_meta["label"].value_counts(normalize=True))
```

三项检查：
1. 嵌入行数和元数据行数一致（480 = 480）
2. 嵌入中没有 NaN
3. 标签分布接近 50/50（50.8% vs 49.2%），说明数据集是平衡的，不需要特殊处理类别不平衡

---

## Cell 15：Logistic Regression 基线 + 构建异构图

这是最长的一个 Cell，做了两件事。

### 第一件：Logistic Regression 基线

#### 时间序列切分

```python
dates = meta["date"]
t80 = dates.iloc[int(len(dates) * 0.8)]
t90 = dates.iloc[int(len(dates) * 0.9)]
train_mask = (dates <= t80)
val_mask   = ((dates > t80) & (dates <= t90))
test_mask  = (dates > t90)
```

按日期排序后，前 80% 做训练集（385 条），中间 10% 做验证集（48 条），最后 10% 做测试集（47 条）。**关键：不能随机切分！** 因为金融数据有时间顺序，用未来的数据训练再预测过去是作弊（数据泄漏）。

#### 训练基线模型

```python
clf = LogisticRegression(class_weight="balanced", solver="saga", C=2.0, max_iter=500)
clf.fit(X_train, y_train)
```

Logistic Regression 是最简单的分类器，直接用 384 维文本嵌入预测涨跌，**完全不用图结构**。这是一个"下界"——如果 GNN 连这个都打不过，就说明图结构没用。结果：Val AUC 0.52（约等于随机猜），Test AUC 0.62（有点信号但不强）。

### 第二件：构建异构图（HeteroData）

```python
data = HeteroData()
data["news"].x = torch.from_numpy(emb)          # 480个新闻节点，384维特征
data["news"].y = torch.from_numpy(y).long()      # 标签：0=跌，1=涨
data["stock"].x = torch.eye(num_stocks)           # 9个股票节点，one-hot特征
data["news", "relates_to", "stock"].edge_index = torch.stack([src, dst])  # 480条边
```

详细讲解见下方「深入讲解」部分。

---

## Cell 16：GraphSAGE 训练

### 添加反向边

```python
data = T.ToUndirected()(data).to(device)
```

`ToUndirected()` 自动添加反向边（stock → news），使消息可以双向传播。

### 模型定义

```python
class GNN(torch.nn.Module):
    def __init__(self, hidden_channels=32):
        self.conv1 = SAGEConv((-1, -1), hidden_channels)
        self.conv2 = SAGEConv((-1, -1), hidden_channels)
        self.lin = torch.nn.Linear(hidden_channels, 1)
```

GraphSAGE 和前面的 GCN 类似，但更灵活：
- `(-1, -1)` 表示"让 PyG 自动推断输入维度"。因为异构图中不同类型节点维度不同（news=384，stock=9），不能写死。
- 两层 SAGE → 每个新闻节点最终能"看到"：自己的文本 + 同股票的其他新闻（通过 stock 节点中转两跳）
- `0.4 dropout`：训练时随机丢掉 40% 的神经元，防过拟合

### 转为异构模型

```python
model = to_hetero(GNN(), data.metadata(), aggr='mean')
```

`to_hetero()` 是 PyG 的魔法——自动把同构 GNN 复制成异构版本，为每种边类型创建独立的权重。`aggr='mean'` 表示如果一个节点收到多种类型边的消息，取平均。

### 训练循环 + Early Stopping

```python
for epoch in range(1, 201):
    loss = train_step()
    val_auc = eval_auc(data['news'].val_mask)
    if val_auc > best_val:
        best_val = val_auc
        best_state = model.state_dict()
        stale = 0
    else:
        stale += 1
    if stale >= patience:
        break
```

- 每个 epoch：前向传播 → 算 loss → 反向传播更新权重
- 每次都检查 validation AUC，如果比之前最好的还好就保存模型
- 如果连续 15 个 epoch 都没有提升，就停止（防止过拟合）
- 最终在 epoch 21 就停了，Best Val AUC = 0.6111，Test AUC = 0.6426

**对比基线：** GraphSAGE 的 Test AUC（0.64）比 Logistic Regression（0.62）高了 3.4%。虽然幅度不大，但考虑到只有 480 条数据和 9 只股票，图结构确实带来了额外的预测能力。

---

## 总结：整个数据流

```
Yahoo Finance 价格 → 相关性矩阵 → 图（Phase 1 分析）
                                      ↓
Factiva 新闻 → 清洗/匹配 → SentenceTransformer 编码 → 异构图 → GraphSAGE → 预测涨跌
```

---

# 深入讲解：HeteroData 与 GraphSAGE

## HeteroData 是怎么建立的？

你可以把 HeteroData 想象成一个**容器**，里面装了不同种类的"东西"和"关系"。

建立过程就是往容器里一样一样地放东西：

```
第一步：放入 480 个新闻节点
    data["news"].x = 384维文本嵌入    ← 每篇新闻的"内容"
    data["news"].y = 0或1             ← 标签（次日跌=0，涨=1）
    data["news"].train/val/test_mask  ← 哪些用于训练/验证/测试

第二步：放入 9 个股票节点
    data["stock"].x = 9维 one-hot     ← 身份标识（占位符）
    例：CFG=[1,0,0,0,0,0,0,0,0]
        TFC=[0,1,0,0,0,0,0,0,0]  ...以此类推

第三步：放入边（关系）
    data["news", "relates_to", "stock"].edge_index
    = 480条边，每篇新闻连向它提到的那只股票
```

画出来大概是这样：

```
新闻A(关于CFG) ──→ CFG
新闻B(关于CFG) ──→ CFG
新闻C(关于TFC) ──→ TFC
新闻D(关于TFC) ──→ TFC
新闻E(关于CFG) ──→ CFG
...共480条这样的边
```

然后 `ToUndirected()` 自动加了反向边：

```
CFG ──→ 新闻A
CFG ──→ 新闻B
CFG ──→ 新闻E
TFC ──→ 新闻C
TFC ──→ 新闻D
```

### HeteroData 里最终包含的完整信息

| 内容 | 具体 |
|------|------|
| 新闻节点 | 480 个，每个有 384 维特征 + 涨跌标签 |
| 股票节点 | 9 个，每个有 9 维 one-hot 特征 |
| 正向边 | news → stock，480 条 |
| 反向边 | stock → news，480 条（自动生成） |
| 训练掩码 | 385 / 48 / 47 的时间序列切分 |

---

## GraphSAGE 的两层是什么意思？

**每一层做一件事：让每个节点"看一眼"自己的邻居，然后更新自己的表示。**

想象一个传话游戏：

### 第一层（1 跳）：每个节点收集直接邻居的信息

```
新闻A ← 看看自己连着谁？连着 CFG
         把 CFG 的特征拿过来，和自己的特征混合
         → 新闻A 现在"知道"自己关联的是 CFG 这只股票

CFG   ← 看看自己连着谁？连着 新闻A、新闻B、新闻E
         把这三篇新闻的特征取平均，和自己的特征混合
         → CFG 现在"知道"关于自己的所有新闻说了什么
```

**第一层结束后：**
- 每个新闻节点知道了"我说的是哪只股票"
- 每个股票节点知道了"关于我的所有新闻说了什么"

### 第二层（2 跳）：再收集一轮

```
新闻A ← 再看一眼 CFG
         但此时 CFG 已经不是原来的 CFG 了！
         CFG 在第一层已经吸收了 新闻A、B、E 的信息
         → 新闻A 现在间接"看到了"新闻B和新闻E！
```

---

## "转两跳"的意义

```
新闻A → CFG → 新闻B
  ↑              ↑
 第1跳          第2跳
```

新闻 A 和新闻 B **没有直接相连**，但通过 CFG 这个中继站，经过两跳，新闻 A 就能间接获取新闻 B 的信息。

**用人话说就是：** "我是一篇关于 CFG 的新闻，通过两层传播，我现在知道了关于 CFG 的所有其他新闻都说了什么。如果其他新闻大多是利好，我可能也该预测涨。"

---

## 具体的信息流示例

用一个完整例子走一遍：

```
初始状态：
  新闻A.特征 = [0.12, -0.34, ..., 0.56]  (384维，来自SentenceTransformer)
  新闻B.特征 = [0.08,  0.21, ..., -0.11] (384维)
  CFG.特征   = [1, 0, 0, 0, 0, 0, 0, 0, 0] (9维 one-hot)

═══ 第一层 SAGE ═══

  CFG 收集邻居(新闻A, 新闻B):
    聚合 = mean(新闻A.特征, 新闻B.特征)
    CFG.新特征 = Linear([CFG.特征 拼接 聚合]) → 32维

  新闻A 收集邻居(CFG):
    聚合 = CFG.特征
    新闻A.新特征 = Linear([新闻A.特征 拼接 聚合]) → 32维

  → ReLU激活 → Dropout(40%)

═══ 第二层 SAGE ═══

  新闻A 再次收集邻居(CFG):
    但此时 CFG 的32维特征已经融合了新闻B的信息！
    新闻A.最终特征 = Linear([...]) → 32维

═══ 分类头 ═══

  新闻A.预测 = Linear(32维 → 1维) → sigmoid → 概率
  如果概率 > 0.5 → 预测涨，否则预测跌
```

---

## 为什么图结构有用？

**没有图结构时（Logistic Regression）：** 每篇新闻孤立地做预测。"CFG 裁员了" → 猜涨跌。

**有图结构时（GraphSAGE）：** 每篇新闻结合同一只股票的所有其他新闻做预测。"CFG 裁员了" + "CFG 最近还有三篇利好新闻" → 更准地猜涨跌。

这就是为什么 GraphSAGE（0.64）比 Logistic Regression（0.62）强——它多了**上下文**。图结构让模型看到的不只是"这一篇文章"，而是"关于这只股票的所有信息的汇总"。
